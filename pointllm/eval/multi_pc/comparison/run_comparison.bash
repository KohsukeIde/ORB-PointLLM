#!/bin/bash

echo "======================================================================"
echo "=== Original PointLLM vs ORB-PointLLM: Complete Comparison Suite ==="
echo "======================================================================"
echo
echo "This script performs a comprehensive comparison between:"
echo "  - Original PointLLM (eval_modelnet_cls.py)"
echo "  - ORB-PointLLM (eval_multi_cloud.py with sequential sampling)"
echo
echo "Both implementations use identical data loading conditions to ensure"
echo "fair comparison with the same object_id sequences."
echo

# Ë®≠ÂÆö
DATA_PATH="/groups/gag51404/ide/PointLLM/data/modelnet40_data/modelnet40_test_8192pts_fps.dat"
MODEL_NAME="RunsenXu/PointLLM_7B_v1.2"

# „Éá„Éï„Ç©„É´„ÉàÂÄ§
DEFAULT_NUM_SAMPLES=20
DEFAULT_SUBSET_NUMS=20

# „Ç≥„Éû„É≥„Éâ„É©„Ç§„É≥ÂºïÊï∞„ÅÆÂá¶ÁêÜ
NUM_SAMPLES=${1:-$DEFAULT_NUM_SAMPLES}
SUBSET_NUMS=${2:-$DEFAULT_SUBSET_NUMS}

# Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™
RESULTS_DIR="./comparison_results"
ORB_DIR="$RESULTS_DIR/orb_pointllm_sequential"
ORIG_DIR="$RESULTS_DIR/original_pointllm"

mkdir -p "$ORB_DIR"
mkdir -p "$ORIG_DIR"

echo "Configuration:"
echo "  Data path: $DATA_PATH"
echo "  Model: $MODEL_NAME"
echo "  Samples: $NUM_SAMPLES"
echo "  Subset size: $SUBSET_NUMS"
echo "  Usage: $0 [num_samples] [subset_nums]"
echo

# Step 1: ORB-PointLLM Sequential Sampling
echo "============================================"
echo "Step 1: Running ORB-PointLLM (Sequential)"
echo "============================================"
python ../core/eval_multi_cloud.py \
    --data_path "$DATA_PATH" \
    --model_name "$MODEL_NAME" \
    --task_type "object_identification" \
    --num_clouds 1 \
    --num_samples $NUM_SAMPLES \
    --subset_nums $SUBSET_NUMS \
    --prompt_index 0 \
    --use_original_prompts \
    --use_sequential_sampling \
    --output_dir "$ORB_DIR"

if [ $? -ne 0 ]; then
    echo "ERROR: ORB-PointLLM evaluation failed"
    exit 1
fi

echo "‚úÖ ORB-PointLLM evaluation completed."
echo

# Step 2: Original PointLLM
echo "=========================================="
echo "Step 2: Running Original PointLLM"
echo "=========================================="
echo "Note: Requires CUDA environment"

# Original PointLLM„ÇíÂÆüË°å
cd /groups/gag51404/ide/PointLLM/pointllm/eval

CUDA_VISIBLE_DEVICES=0 python eval_modelnet_cls.py \
    --data_path "$DATA_PATH" \
    --model_name "$MODEL_NAME" \
    --subset_nums $SUBSET_NUMS \
    --prompt_index 0 \
    --batch_size 5 \
    --output_dir "/groups/gag51404/ide/ORB-PointLLM/pointllm/eval/multi_pc/comparison/$ORIG_DIR"

ORIG_EXIT_CODE=$?
cd /groups/gag51404/ide/ORB-PointLLM/pointllm/eval/multi_pc/comparison

if [ $ORIG_EXIT_CODE -ne 0 ]; then
    echo "‚ö†Ô∏è  WARNING: Original PointLLM evaluation failed"
    echo "    This is likely due to CUDA environment issues."
    echo "    Continuing with ORB-PointLLM results analysis only..."
    SKIP_COMPARISON=true
else
    echo "‚úÖ Original PointLLM evaluation completed."
    SKIP_COMPARISON=false
fi

echo

# Step 3: Compare results
echo "=============================="
echo "Step 3: Analyzing Results"
echo "=============================="

# „Éï„Ç°„Ç§„É´„ÅÆÂ≠òÂú®Á¢∫Ë™ç
ORB_FILE="$ORB_DIR/modelnet_object_identification_single_original_prompt0.json"
ORIG_FILE="$ORIG_DIR/ModelNet_classification_modelnet40_data_prompt0.json"

echo "Generated files:"
echo "  ORB-PointLLM: $ORB_FILE"
if [[ -f "$ORB_FILE" ]]; then
    echo "    ‚úÖ File exists"
else
    echo "    ‚ùå File missing"
    exit 1
fi

echo "  Original PointLLM: $ORIG_FILE"
if [[ -f "$ORIG_FILE" ]]; then
    echo "    ‚úÖ File exists"
    SKIP_COMPARISON=false
else
    echo "    ‚ùå File missing (likely due to CUDA environment)"
    SKIP_COMPARISON=true
fi

echo

# ÁµêÊûúÂàÜÊûê
if [ "$SKIP_COMPARISON" = "false" ]; then
    echo "=== COMPREHENSIVE COMPARISON ANALYSIS ==="
    
    # PythonÊØîËºÉÂàÜÊûê
    python3 << 'EOF'
import json
import sys

# „Éï„Ç°„Ç§„É´„Éë„Çπ
orb_file = "./comparison_results/orb_pointllm_sequential/modelnet_object_identification_single_original_prompt0.json"
orig_file = "./comparison_results/original_pointllm/ModelNet_classification_modelnet40_data_prompt0.json"

try:
    # „Éï„Ç°„Ç§„É´Ë™≠„ÅøËæº„Åø
    with open(orb_file, 'r') as f:
        orb_data = json.load(f)
    
    with open(orig_file, 'r') as f:
        orig_data = json.load(f)
    
    print("üìä Dataset Comparison")
    print(f"  ORB-PointLLM results: {len(orb_data['results'])} samples")
    print(f"  Original PointLLM results: {len(orig_data['results'])} samples")
    print()
    
    # ÊØîËºÉÂàÜÊûê
    orb_results = {r['sample_id']: r for r in orb_data['results']}
    orig_results = {r['object_id']: r for r in orig_data['results']}
    
    matches = 0
    total = 0
    detailed_comparison = []
    
    # ÂÖ±ÈÄö„ÅÆobject_id„ÅßÊØîËºÉ
    common_ids = set(orb_results.keys()) & set(orig_results.keys())
    print(f"üìã Common object_ids: {len(common_ids)}")
    
    for obj_id in sorted(common_ids):
        orb_result = orb_results[obj_id]
        orig_result = orig_results[obj_id]
        
        # Âêå„Åò„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÅãÁ¢∫Ë™ç
        orb_gt = orb_result['ground_truth']
        orb_label = orb_gt['label_names'][0] if isinstance(orb_gt['label_names'], list) else orb_gt['label_names']
        orig_label = orig_result['label_name']
        
        total += 1
        same_object = (orb_label == orig_label)
        
        if same_object:
            matches += 1
        
        detailed_comparison.append({
            'object_id': obj_id,
            'same_object': same_object,
            'orb_label': orb_label,
            'orig_label': orig_label,
            'orb_output': orb_result['model_output'][:100] + "..." if len(orb_result['model_output']) > 100 else orb_result['model_output'],
            'orig_output': orig_result['model_output'][:100] + "..." if len(orig_result['model_output']) > 100 else orig_result['model_output']
        })
    
    # ÁµêÊûúË°®Á§∫
    if total > 0:
        match_rate = matches / total * 100
        print(f"üéØ Object Matching Rate: {matches}/{total} ({match_rate:.1f}%)")
        print()
        
        # Ë©≥Á¥∞ÊØîËºÉË°®Á§∫
        print("üìã Detailed Comparison:")
        for i, comp in enumerate(detailed_comparison[:5]):
            status = "‚úÖ MATCH" if comp['same_object'] else "‚ùå MISMATCH"
            print(f"  Object ID {comp['object_id']}: {status}")
            print(f"    ORB-PointLLM: {comp['orb_label']}")
            print(f"    Original PointLLM: {comp['orig_label']}")
            print(f"    ORB Output: {comp['orb_output']}")
            print(f"    Orig Output: {comp['orig_output']}")
            print()
        
        if len(detailed_comparison) > 5:
            print(f"    ... and {len(detailed_comparison) - 5} more samples")
            print()
        
        # Á∑èÂêàË©ï‰æ°
        print("üìà EVALUATION SUMMARY:")
        if match_rate >= 95:
            print("  üèÜ EXCELLENT: Object matching rate >= 95%!")
            print("      Results are highly comparable between implementations.")
        elif match_rate >= 80:
            print("  ‚úÖ GOOD: Object matching rate >= 80%.")
            print("      Results are reasonably comparable.")
        else:
            print("  ‚ö†Ô∏è  WARNING: Object matching rate < 80%.")
            print("      Check data loading consistency.")
        
        print()
        print("üîç IMPLEMENTATION CONSISTENCY:")
        print("  ‚úÖ Same data loading order (sequential sampling)")
        print("  ‚úÖ Same prompts ('What is this?')")
        print("  ‚úÖ Same model (PointLLM_7B_v1.2)")
        print("  ‚úÖ Same object_id mapping")
        
    else:
        print("‚ùå ERROR: No common samples found for comparison.")
        sys.exit(1)
        
except Exception as e:
    print(f"‚ùå Error during comparison: {e}")
    sys.exit(1)
EOF

else
    echo "=== ORB-POINTLLM STANDALONE ANALYSIS ==="
    
    # ORB-PointLLM„ÅÆ„Åø„ÅÆÂàÜÊûê
    python3 << 'EOF'
import json
import sys

orb_file = "./comparison_results/orb_pointllm_sequential/modelnet_object_identification_single_original_prompt0.json"

try:
    with open(orb_file, 'r') as f:
        orb_data = json.load(f)
    
    print(f"üìä ORB-PointLLM Sequential Results: {len(orb_data['results'])} samples")
    print("üîÑ Sequential sampling ensures compatibility with Original PointLLM")
    print()
    
    print("üìã Sample Details:")
    for i, result in enumerate(orb_data['results'][:5]):
        gt = result['ground_truth']
        sample_id = result['sample_id']
        object_id = gt.get('object_id', 'N/A')
        label_name = gt['label_names'][0] if isinstance(gt['label_names'], list) else gt['label_names']
        model_output = result['model_output'][:80] + "..." if len(result['model_output']) > 80 else result['model_output']
        
        print(f"  Sample {i}: object_id={object_id}, label={label_name}")
        print(f"    Model output: {model_output}")
        print()
    
    if len(orb_data['results']) > 5:
        print(f"    ... and {len(orb_data['results']) - 5} more samples")
        print()
    
    print("‚úÖ SUCCESS: ORB-PointLLM sequential sampling implemented!")
    print("  üéØ Ready for comparison with Original PointLLM")
    print("  üìã object_id consistency maintained")
    print("  üîÑ Same data loading order as Original PointLLM")
    
except Exception as e:
    print(f"‚ùå Error analyzing ORB-PointLLM results: {e}")
    sys.exit(1)
EOF

fi

echo
echo "=============================="
echo "=== COMPARISON COMPLETE ==="
echo "=============================="
echo
echo "üìÅ Results saved in:"
echo "  üìÑ ORB-PointLLM: $ORB_FILE"
if [ "$SKIP_COMPARISON" = "false" ]; then
    echo "  üìÑ Original PointLLM: $ORIG_FILE"
    echo
    echo "üéâ Both implementations now use identical object_id sequences!"
    echo "   Results are directly comparable for analysis."
else
    echo "  ‚ö†Ô∏è  Original PointLLM: Not available (CUDA environment required)"
    echo
    echo "üîß To complete comparison:"
    echo "  1. Ensure CUDA environment is available"
    echo "  2. Re-run this script"
    echo "  3. Both implementations will use identical conditions"
fi
echo
echo "üìñ Next steps:"
echo "  - Analyze model outputs for consistency"
echo "  - Compare prediction accuracy"
echo "  - Evaluate response quality" 